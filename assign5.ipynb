{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another popular text analysis technique is called topic modeling. The ultimate goal of topic modeling is to find various topics that are present in your corpus. Each document in the corpus will be made up of at least one topic, if not multiple topics.\n",
    "\n",
    "In this notebook, we will be covering the steps on how to do **Latent Dirichlet Allocation (LDA)**, which is one of many topic modeling techniques. It was specifically designed for text data.\n",
    "\n",
    "To use a topic modeling technique, you need to provide (1) a document-term matrix and (2) the number of topics you would like the algorithm to pick up.\n",
    "\n",
    "Once the topic modeling technique is applied, your job as a human is to interpret the results and see if the mix of words in each topic make sense. If they don't make sense, you can try changing up the number of topics, the terms in the document-term matrix, model parameters, or even try a different model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling - Attempt #1 (All Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ab</th>\n",
       "      <th>abilities</th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>absorbed</th>\n",
       "      <th>absorbing</th>\n",
       "      <th>abstract</th>\n",
       "      <th>academia</th>\n",
       "      <th>academic</th>\n",
       "      <th>...</th>\n",
       "      <th>young</th>\n",
       "      <th>younger</th>\n",
       "      <th>youpaul</th>\n",
       "      <th>youprobablycohentannoudji</th>\n",
       "      <th>youre</th>\n",
       "      <th>yourselfyouve</th>\n",
       "      <th>yoursrobert</th>\n",
       "      <th>yousvante</th>\n",
       "      <th>youve</th>\n",
       "      <th>youwatch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Anne</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Claudia</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gerhard</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Katalin</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Leland</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Paul</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pierre</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Richard</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Robert</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Svante</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 3875 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ab  abilities  ability  able  absolutely  absorbed  absorbing  \\\n",
       "Anne      1          0        0     1           2         0          0   \n",
       "Claudia   1          0        0     3           1         0          0   \n",
       "Gerhard   2          0        0     3           0         0          0   \n",
       "Katalin   1          0        0     0           0         0          0   \n",
       "Leland    1          1        2     6           0         0          0   \n",
       "Paul      1          0        1     0           0         0          0   \n",
       "Pierre    1          0        0     0           1         0          0   \n",
       "Richard   1          0        1    11           4         0          0   \n",
       "Robert    1          0        0     0           1         0          0   \n",
       "Svante    1          0        1     1           0         1          1   \n",
       "\n",
       "         abstract  academia  academic  ...  young  younger  youpaul  \\\n",
       "Anne            0         1         0  ...      4        0        0   \n",
       "Claudia         4         0         1  ...      3        0        0   \n",
       "Gerhard         0         0         0  ...      0        0        0   \n",
       "Katalin         0         0         0  ...      5        0        0   \n",
       "Leland          0         0         0  ...      0        0        0   \n",
       "Paul            0         1         3  ...      6        0        1   \n",
       "Pierre          0         0         0  ...      0        0        0   \n",
       "Richard         1         0         0  ...      2        0        0   \n",
       "Robert          0         0         0  ...      0        0        0   \n",
       "Svante          0         0         0  ...      1        1        0   \n",
       "\n",
       "         youprobablycohentannoudji  youre  yourselfyouve  yoursrobert  \\\n",
       "Anne                             0      0              0            0   \n",
       "Claudia                          0      4              0            0   \n",
       "Gerhard                          0      1              0            0   \n",
       "Katalin                          0      0              0            0   \n",
       "Leland                           0      6              1            0   \n",
       "Paul                             0     10              0            0   \n",
       "Pierre                           1      1              0            0   \n",
       "Richard                          0     11              0            0   \n",
       "Robert                           0      1              0            1   \n",
       "Svante                           0      3              0            0   \n",
       "\n",
       "         yousvante  youve  youwatch  \n",
       "Anne             0      0         0  \n",
       "Claudia          0      1         0  \n",
       "Gerhard          0      2         0  \n",
       "Katalin          0      1         0  \n",
       "Leland           0      5         0  \n",
       "Paul             0      5         1  \n",
       "Pierre           0      0         0  \n",
       "Richard          0      6         1  \n",
       "Robert           0      0         1  \n",
       "Svante           1      0         0  \n",
       "\n",
       "[10 rows x 3875 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's read in our document-term matrix\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "data = pd.read_pickle('dtm_stop.pkl')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary modules for LDA with gensim\n",
    "# Terminal / Anaconda Navigator: conda install -c conda-forge gensim\n",
    "from gensim import matutils, models\n",
    "import scipy.sparse\n",
    "\n",
    "# import logging\n",
    "# logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Anne</th>\n",
       "      <th>Claudia</th>\n",
       "      <th>Gerhard</th>\n",
       "      <th>Katalin</th>\n",
       "      <th>Leland</th>\n",
       "      <th>Paul</th>\n",
       "      <th>Pierre</th>\n",
       "      <th>Richard</th>\n",
       "      <th>Robert</th>\n",
       "      <th>Svante</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ab</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abilities</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ability</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>able</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>absolutely</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Anne  Claudia  Gerhard  Katalin  Leland  Paul  Pierre  Richard  \\\n",
       "ab             1        1        2        1       1     1       1        1   \n",
       "abilities      0        0        0        0       1     0       0        0   \n",
       "ability        0        0        0        0       2     1       0        1   \n",
       "able           1        3        3        0       6     0       0       11   \n",
       "absolutely     2        1        0        0       0     0       1        4   \n",
       "\n",
       "            Robert  Svante  \n",
       "ab               1       1  \n",
       "abilities        0       0  \n",
       "ability          0       1  \n",
       "able             0       1  \n",
       "absolutely       1       0  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One of the required inputs is a term-document matrix\n",
    "tdm = data.transpose()\n",
    "tdm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're going to put the term-document matrix into a new gensim format, from df --> sparse matrix --> gensim corpus\n",
    "sparse_counts = scipy.sparse.csr_matrix(tdm)\n",
    "corpus = matutils.Sparse2Corpus(sparse_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gensim also requires dictionary of the all terms and their respective location in the term-document matrix\n",
    "cv = pickle.load(open(\"cv_stop.pkl\", \"rb\"))\n",
    "id2word = dict((v, k) for k, v in cv.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the corpus (term-document matrix) and id2word (dictionary of location: term), we need to specify two other parameters - the number of topics and the number of passes. Let's start the number of topics at 2, see if the results make sense, and increase the number from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.008*\"thats\" + 0.008*\"people\" + 0.008*\"think\" + 0.007*\"work\" + 0.007*\"just\" + 0.006*\"like\" + 0.006*\"things\" + 0.006*\"economics\" + 0.006*\"know\" + 0.006*\"dont\"'),\n",
       " (1,\n",
       "  '0.011*\"think\" + 0.011*\"people\" + 0.007*\"really\" + 0.007*\"just\" + 0.007*\"time\" + 0.006*\"like\" + 0.006*\"nobel\" + 0.006*\"doing\" + 0.005*\"thats\" + 0.005*\"know\"')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now that we have the corpus (term-document matrix) and id2word (dictionary of location: term),\n",
    "# we need to specify two other parameters as well - the number of topics and the number of passes\n",
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=2, passes=10)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.012*\"think\" + 0.012*\"people\" + 0.008*\"just\" + 0.008*\"time\" + 0.007*\"really\" + 0.007*\"like\" + 0.007*\"doing\" + 0.006*\"nobel\" + 0.006*\"say\" + 0.006*\"roberts\"'),\n",
       " (1,\n",
       "  '0.010*\"thats\" + 0.009*\"think\" + 0.008*\"ertl\" + 0.007*\"things\" + 0.007*\"work\" + 0.007*\"just\" + 0.007*\"really\" + 0.006*\"people\" + 0.006*\"hartwell\" + 0.006*\"like\"'),\n",
       " (2,\n",
       "  '0.016*\"economics\" + 0.010*\"people\" + 0.008*\"women\" + 0.006*\"know\" + 0.006*\"goldin\" + 0.006*\"work\" + 0.005*\"dont\" + 0.005*\"thats\" + 0.004*\"think\" + 0.004*\"just\"')]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LDA for num_topics = 3\n",
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=3, passes=10)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.001*\"people\" + 0.000*\"thats\" + 0.000*\"work\" + 0.000*\"really\" + 0.000*\"think\" + 0.000*\"things\" + 0.000*\"just\" + 0.000*\"dont\" + 0.000*\"know\" + 0.000*\"time\"'),\n",
       " (1,\n",
       "  '0.010*\"people\" + 0.009*\"think\" + 0.008*\"roberts\" + 0.008*\"really\" + 0.008*\"just\" + 0.007*\"ertl\" + 0.006*\"economics\" + 0.006*\"know\" + 0.006*\"doing\" + 0.006*\"time\"'),\n",
       " (2,\n",
       "  '0.011*\"people\" + 0.011*\"think\" + 0.009*\"just\" + 0.009*\"greengard\" + 0.008*\"really\" + 0.007*\"work\" + 0.007*\"thats\" + 0.007*\"things\" + 0.006*\"time\" + 0.006*\"hartwell\"'),\n",
       " (3,\n",
       "  '0.012*\"think\" + 0.010*\"like\" + 0.009*\"important\" + 0.009*\"people\" + 0.008*\"nobel\" + 0.008*\"thats\" + 0.007*\"different\" + 0.006*\"things\" + 0.006*\"say\" + 0.006*\"science\"')]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LDA for num_topics = 4\n",
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=4, passes=10)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These topics aren't looking too great. We've tried modifying our parameters. Let's try modifying our terms list as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling - Attempt #2 (Nouns Only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One popular trick is to look only at terms that are from one part of speech (only nouns, only adjectives, etc.). Check out the UPenn tag set: https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a function to pull out nouns from a string of text\n",
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "def nouns(text):\n",
    "    '''Given a string of text, tokenize the text and pull out only the nouns.'''\n",
    "    is_noun = lambda pos: pos[:2] == 'NN'\n",
    "    tokenized = word_tokenize(text)\n",
    "    all_nouns = [word for (word, pos) in pos_tag(tokenized) if is_noun(pos)] \n",
    "    return ' '.join(all_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>awardee</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Anne</th>\n",
       "      <td>the nobel prize in physics  agostiniferenc kra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Claudia</th>\n",
       "      <td>the sveriges riksbank prize in economic scienc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gerhard</th>\n",
       "      <td>the nobel prize in chemistry  ertlshare thissh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Katalin</th>\n",
       "      <td>the nobel prize in physiology or medicine  kar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Leland</th>\n",
       "      <td>the nobel prize in physiology or medicine  har...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Paul</th>\n",
       "      <td>the nobel prize in physiology or medicine  car...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pierre</th>\n",
       "      <td>the nobel prize in physics  agostiniferenc kra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Richard</th>\n",
       "      <td>the nobel prize in physiology or medicine  j r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Robert</th>\n",
       "      <td>the nobel prize in chemistry  j lefkowitzbrian...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Svante</th>\n",
       "      <td>the nobel prize in physiology or medicine  pÃ¤Ã¤...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   awardee\n",
       "Anne     the nobel prize in physics  agostiniferenc kra...\n",
       "Claudia  the sveriges riksbank prize in economic scienc...\n",
       "Gerhard  the nobel prize in chemistry  ertlshare thissh...\n",
       "Katalin  the nobel prize in physiology or medicine  kar...\n",
       "Leland   the nobel prize in physiology or medicine  har...\n",
       "Paul     the nobel prize in physiology or medicine  car...\n",
       "Pierre   the nobel prize in physics  agostiniferenc kra...\n",
       "Richard  the nobel prize in physiology or medicine  j r...\n",
       "Robert   the nobel prize in chemistry  j lefkowitzbrian...\n",
       "Svante   the nobel prize in physiology or medicine  pÃ¤Ã¤..."
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the cleaned data, before the CountVectorizer step\n",
    "data_clean = pd.read_pickle('data_clean.pkl')\n",
    "data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/krishuppal/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>awardee</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Anne</th>\n",
       "      <td>prize physics krauszanne thisshare facebook tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Claudia</th>\n",
       "      <td>sveriges sciences memory nobel goldinshare thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gerhard</th>\n",
       "      <td>prize chemistry ertlshare thisshare facebook t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Katalin</th>\n",
       "      <td>prize physiology medicine karikÃ³drew facebook ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Leland</th>\n",
       "      <td>prize physiology medicine hartwelltim huntsir ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Paul</th>\n",
       "      <td>prize physiology medicine carlssonpaul greenga...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pierre</th>\n",
       "      <td>prize physics krauszanne thisshare facebook tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Richard</th>\n",
       "      <td>prize physiology medicine j sharpshare thissha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Robert</th>\n",
       "      <td>prize chemistry j kobilkashare thisshare faceb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Svante</th>\n",
       "      <td>prize physiology medicine pÃ¤Ã¤boshare thisshare...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   awardee\n",
       "Anne     prize physics krauszanne thisshare facebook tr...\n",
       "Claudia  sveriges sciences memory nobel goldinshare thi...\n",
       "Gerhard  prize chemistry ertlshare thisshare facebook t...\n",
       "Katalin  prize physiology medicine karikÃ³drew facebook ...\n",
       "Leland   prize physiology medicine hartwelltim huntsir ...\n",
       "Paul     prize physiology medicine carlssonpaul greenga...\n",
       "Pierre   prize physics krauszanne thisshare facebook tr...\n",
       "Richard  prize physiology medicine j sharpshare thissha...\n",
       "Robert   prize chemistry j kobilkashare thisshare faceb...\n",
       "Svante   prize physiology medicine pÃ¤Ã¤boshare thisshare..."
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the nouns function to the transcripts to filter only on nouns\n",
    "data_nouns = pd.DataFrame(data_clean.awardee.apply(nouns))\n",
    "data_nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ab</th>\n",
       "      <th>abilities</th>\n",
       "      <th>ability</th>\n",
       "      <th>academia</th>\n",
       "      <th>academic</th>\n",
       "      <th>academics</th>\n",
       "      <th>account</th>\n",
       "      <th>accumulation</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>achievements</th>\n",
       "      <th>...</th>\n",
       "      <th>yeswhats</th>\n",
       "      <th>york</th>\n",
       "      <th>youd</th>\n",
       "      <th>youits</th>\n",
       "      <th>youleland</th>\n",
       "      <th>youll</th>\n",
       "      <th>youprobablycohentannoudji</th>\n",
       "      <th>yourselfyouve</th>\n",
       "      <th>yousvante</th>\n",
       "      <th>youve</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Anne</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Claudia</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gerhard</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Katalin</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Leland</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Paul</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pierre</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Richard</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Robert</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Svante</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 2096 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ab  abilities  ability  academia  academic  academics  account  \\\n",
       "Anne      0          0        0         1         0          0        0   \n",
       "Claudia   1          0        0         0         1          1        3   \n",
       "Gerhard   1          0        0         0         0          0        0   \n",
       "Katalin   1          0        0         0         0          0        0   \n",
       "Leland    1          1        2         0         0          2        0   \n",
       "Paul      1          0        1         1         0          0        0   \n",
       "Pierre    1          0        0         0         0          0        0   \n",
       "Richard   1          0        1         0         0          0        0   \n",
       "Robert    1          0        0         0         0          0        0   \n",
       "Svante    1          0        1         0         0          0        0   \n",
       "\n",
       "         accumulation  accuracy  achievements  ...  yeswhats  york  youd  \\\n",
       "Anne                0         0             1  ...         0     0     0   \n",
       "Claudia             0         0             1  ...         0     2     0   \n",
       "Gerhard             0         0             1  ...         1     0     0   \n",
       "Katalin             0         0             1  ...         0     0     0   \n",
       "Leland              1         1             1  ...         0     0     0   \n",
       "Paul                0         0             1  ...         0     0     1   \n",
       "Pierre              0         0             1  ...         0     0     0   \n",
       "Richard             0         0             1  ...         0     0     1   \n",
       "Robert              0         0             1  ...         0     0     0   \n",
       "Svante              0         0             1  ...         0     0     0   \n",
       "\n",
       "         youits  youleland  youll  youprobablycohentannoudji  yourselfyouve  \\\n",
       "Anne          0          0      0                          0              0   \n",
       "Claudia       0          0      1                          0              0   \n",
       "Gerhard       0          0      1                          0              0   \n",
       "Katalin       0          0      0                          0              0   \n",
       "Leland        0          2      0                          0              1   \n",
       "Paul          1          0      0                          0              0   \n",
       "Pierre        0          0      0                          1              0   \n",
       "Richard       0          0      0                          0              0   \n",
       "Robert        0          0      0                          0              0   \n",
       "Svante        0          0      0                          0              0   \n",
       "\n",
       "         yousvante  youve  \n",
       "Anne             0      0  \n",
       "Claudia          0      0  \n",
       "Gerhard          0      1  \n",
       "Katalin          0      1  \n",
       "Leland           0      2  \n",
       "Paul             0      3  \n",
       "Pierre           0      0  \n",
       "Richard          0      4  \n",
       "Robert           0      0  \n",
       "Svante           1      0  \n",
       "\n",
       "[10 rows x 2096 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new document-term matrix using only nouns\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Re-add the additional stop words since we are recreating the document-term matrix\n",
    "add_stop_words = ['like', 'im', 'know', 'just', 'dont', 'thats', 'right', 'people',\n",
    "                  'youre', 'got', 'gonna', 'time', 'think', 'yeah', 'said']\n",
    "stop_words = list(text.ENGLISH_STOP_WORDS.union(add_stop_words))  # Convert to list\n",
    "\n",
    "# Recreate a document-term matrix with only nouns\n",
    "cvn = CountVectorizer(stop_words=stop_words)\n",
    "data_cvn = cvn.fit_transform(data_nouns.awardee)\n",
    "data_dtmn = pd.DataFrame(data_cvn.toarray(), columns=cvn.get_feature_names_out())  # Use get_feature_names_out() instead\n",
    "data_dtmn.index = data_nouns.index\n",
    "data_dtmn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the gensim corpus\n",
    "corpusn = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(data_dtmn.transpose()))\n",
    "\n",
    "# Create the vocabulary dictionary\n",
    "id2wordn = dict((v, k) for k, v in cvn.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.014*\"things\" + 0.008*\"years\" + 0.008*\"science\" + 0.008*\"ertl\" + 0.007*\"hartwell\" + 0.007*\"students\" + 0.007*\"cell\" + 0.006*\"chemistry\" + 0.006*\"interview\" + 0.006*\"biology\"'),\n",
       " (1,\n",
       "  '0.009*\"things\" + 0.009*\"research\" + 0.009*\"way\" + 0.008*\"greengard\" + 0.008*\"science\" + 0.008*\"interview\" + 0.008*\"economics\" + 0.007*\"prize\" + 0.007*\"students\" + 0.006*\"thing\"')]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's start with 2 topics\n",
    "ldan = models.LdaModel(corpus=corpusn, num_topics=2, id2word=id2wordn, passes=10)\n",
    "ldan.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.016*\"greengard\" + 0.010*\"students\" + 0.009*\"ertl\" + 0.008*\"years\" + 0.008*\"work\" + 0.008*\"research\" + 0.008*\"chemistry\" + 0.007*\"prize\" + 0.007*\"way\" + 0.007*\"drug\"'),\n",
       " (1,\n",
       "  '0.018*\"things\" + 0.011*\"science\" + 0.010*\"interview\" + 0.009*\"year\" + 0.008*\"hartwell\" + 0.008*\"research\" + 0.008*\"transcript\" + 0.008*\"prize\" + 0.008*\"students\" + 0.008*\"thing\"'),\n",
       " (2,\n",
       "  '0.013*\"economics\" + 0.009*\"way\" + 0.008*\"things\" + 0.008*\"women\" + 0.008*\"years\" + 0.008*\"roberts\" + 0.006*\"school\" + 0.006*\"science\" + 0.005*\"experiment\" + 0.005*\"rna\"')]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try topics = 3\n",
    "ldan = models.LdaModel(corpus=corpusn, num_topics=3, id2word=id2wordn, passes=10)\n",
    "ldan.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.017*\"things\" + 0.010*\"science\" + 0.010*\"way\" + 0.010*\"biology\" + 0.008*\"roberts\" + 0.008*\"year\" + 0.008*\"thing\" + 0.008*\"hartwell\" + 0.007*\"dna\" + 0.007*\"cell\"'),\n",
       " (1,\n",
       "  '0.017*\"research\" + 0.016*\"students\" + 0.014*\"group\" + 0.012*\"bit\" + 0.008*\"science\" + 0.008*\"physics\" + 0.008*\"intuition\" + 0.007*\"interview\" + 0.007*\"student\" + 0.006*\"transcript\"'),\n",
       " (2,\n",
       "  '0.022*\"ertl\" + 0.017*\"chemistry\" + 0.011*\"students\" + 0.009*\"science\" + 0.009*\"surface\" + 0.008*\"course\" + 0.008*\"physics\" + 0.007*\"interview\" + 0.006*\"years\" + 0.006*\"question\"'),\n",
       " (3,\n",
       "  '0.012*\"greengard\" + 0.012*\"economics\" + 0.009*\"work\" + 0.009*\"years\" + 0.009*\"women\" + 0.009*\"interview\" + 0.009*\"prize\" + 0.008*\"things\" + 0.008*\"students\" + 0.007*\"school\"')]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try 4 topics\n",
    "ldan = models.LdaModel(corpus=corpusn, num_topics=4, id2word=id2wordn, passes=10)\n",
    "ldan.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling - Attempt #3 (Nouns and Adjectives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a function to pull out nouns from a string of text\n",
    "def nouns_adj(text):\n",
    "    '''Given a string of text, tokenize the text and pull out only the nouns and adjectives.'''\n",
    "    is_noun_adj = lambda pos: pos[:2] == 'NN' or pos[:2] == 'JJ'\n",
    "    tokenized = word_tokenize(text)\n",
    "    nouns_adj = [word for (word, pos) in pos_tag(tokenized) if is_noun_adj(pos)] \n",
    "    return ' '.join(nouns_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>awardee</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Anne</th>\n",
       "      <td>nobel prize physics agostiniferenc krauszanne ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Claudia</th>\n",
       "      <td>sveriges economic sciences memory alfred nobel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gerhard</th>\n",
       "      <td>nobel prize chemistry ertlshare thisshare face...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Katalin</th>\n",
       "      <td>nobel prize physiology medicine karikÃ³drew fac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Leland</th>\n",
       "      <td>nobel prize physiology medicine hartwelltim hu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Paul</th>\n",
       "      <td>nobel prize physiology medicine carlssonpaul g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pierre</th>\n",
       "      <td>nobel prize physics agostiniferenc krauszanne ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Richard</th>\n",
       "      <td>nobel prize physiology medicine j sharpshare t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Robert</th>\n",
       "      <td>nobel prize chemistry j lefkowitzbrian kobilka...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Svante</th>\n",
       "      <td>nobel prize physiology medicine pÃ¤Ã¤boshare thi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   awardee\n",
       "Anne     nobel prize physics agostiniferenc krauszanne ...\n",
       "Claudia  sveriges economic sciences memory alfred nobel...\n",
       "Gerhard  nobel prize chemistry ertlshare thisshare face...\n",
       "Katalin  nobel prize physiology medicine karikÃ³drew fac...\n",
       "Leland   nobel prize physiology medicine hartwelltim hu...\n",
       "Paul     nobel prize physiology medicine carlssonpaul g...\n",
       "Pierre   nobel prize physics agostiniferenc krauszanne ...\n",
       "Richard  nobel prize physiology medicine j sharpshare t...\n",
       "Robert   nobel prize chemistry j lefkowitzbrian kobilka...\n",
       "Svante   nobel prize physiology medicine pÃ¤Ã¤boshare thi..."
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the nouns function to the transcripts to filter only on nouns\n",
    "data_nouns_adj = pd.DataFrame(data_clean.awardee.apply(nouns_adj))\n",
    "data_nouns_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abilities</th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>abstract</th>\n",
       "      <th>academia</th>\n",
       "      <th>academic</th>\n",
       "      <th>academics</th>\n",
       "      <th>accepted</th>\n",
       "      <th>accomplish</th>\n",
       "      <th>accomplished</th>\n",
       "      <th>...</th>\n",
       "      <th>youits</th>\n",
       "      <th>youleland</th>\n",
       "      <th>youll</th>\n",
       "      <th>young</th>\n",
       "      <th>younger</th>\n",
       "      <th>youprobablycohentannoudji</th>\n",
       "      <th>yourselfyouve</th>\n",
       "      <th>yoursrobert</th>\n",
       "      <th>yousvante</th>\n",
       "      <th>youve</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Anne</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Claudia</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gerhard</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Katalin</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Leland</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Paul</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pierre</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Richard</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Robert</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Svante</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 2663 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         abilities  ability  able  abstract  academia  academic  academics  \\\n",
       "Anne             0        0     1         0         1         0          0   \n",
       "Claudia          0        0     3         4         0         1          1   \n",
       "Gerhard          0        0     3         0         0         0          0   \n",
       "Katalin          0        0     0         0         0         0          0   \n",
       "Leland           1        2     6         0         0         0          2   \n",
       "Paul             0        1     0         0         1         3          0   \n",
       "Pierre           0        0     0         0         0         0          0   \n",
       "Richard          0        1    11         1         0         0          0   \n",
       "Robert           0        0     0         0         0         0          0   \n",
       "Svante           0        1     1         0         0         0          0   \n",
       "\n",
       "         accepted  accomplish  accomplished  ...  youits  youleland  youll  \\\n",
       "Anne            0           0             0  ...       0          0      0   \n",
       "Claudia         1           0             0  ...       0          0      1   \n",
       "Gerhard         0           0             0  ...       0          0      1   \n",
       "Katalin         0           1             0  ...       0          0      0   \n",
       "Leland          0           0             0  ...       0          2      1   \n",
       "Paul            1           0             1  ...       1          0      0   \n",
       "Pierre          0           0             0  ...       0          0      0   \n",
       "Richard         0           0             0  ...       0          0      1   \n",
       "Robert          0           0             0  ...       0          0      0   \n",
       "Svante          0           0             0  ...       0          0      0   \n",
       "\n",
       "         young  younger  youprobablycohentannoudji  yourselfyouve  \\\n",
       "Anne         4        0                          0              0   \n",
       "Claudia      3        0                          0              0   \n",
       "Gerhard      0        0                          0              0   \n",
       "Katalin      5        0                          0              0   \n",
       "Leland       0        0                          0              1   \n",
       "Paul         6        0                          0              0   \n",
       "Pierre       0        0                          1              0   \n",
       "Richard      2        0                          0              0   \n",
       "Robert       0        0                          0              0   \n",
       "Svante       1        1                          0              0   \n",
       "\n",
       "         yoursrobert  yousvante  youve  \n",
       "Anne               0          0      0  \n",
       "Claudia            0          0      1  \n",
       "Gerhard            0          0      1  \n",
       "Katalin            0          0      1  \n",
       "Leland             0          0      4  \n",
       "Paul               0          0      3  \n",
       "Pierre             0          0      0  \n",
       "Richard            0          0      4  \n",
       "Robert             1          0      0  \n",
       "Svante             0          1      0  \n",
       "\n",
       "[10 rows x 2663 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new document-term matrix using only nouns and adjectives, also remove common words with max_df\n",
    "cvna = CountVectorizer(stop_words=stop_words, max_df=0.8)\n",
    "data_cvna = cvna.fit_transform(data_nouns_adj.awardee)\n",
    "data_dtmna = pd.DataFrame(data_cvna.toarray(), columns=cvna.get_feature_names_out())  # Use get_feature_names_out() instead\n",
    "data_dtmna.index = data_nouns_adj.index\n",
    "data_dtmna\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the gensim corpus\n",
    "corpusna = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(data_dtmna.transpose()))\n",
    "\n",
    "# Create the vocabulary dictionary\n",
    "id2wordna = dict((v, k) for k, v in cvna.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.007*\"greengard\" + 0.006*\"research\" + 0.006*\"new\" + 0.006*\"students\" + 0.005*\"course\" + 0.005*\"school\" + 0.005*\"chemistry\" + 0.005*\"biology\" + 0.004*\"ertl\" + 0.004*\"interested\"'),\n",
       " (1,\n",
       "  '0.016*\"economics\" + 0.009*\"students\" + 0.008*\"women\" + 0.006*\"research\" + 0.006*\"group\" + 0.006*\"goldin\" + 0.005*\"bit\" + 0.005*\"college\" + 0.004*\"school\" + 0.004*\"diversity\"')]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's start with 2 topics\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=2, id2word=id2wordna, passes=10)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.010*\"greengard\" + 0.007*\"biology\" + 0.006*\"research\" + 0.005*\"new\" + 0.005*\"hartwell\" + 0.005*\"cell\" + 0.005*\"students\" + 0.005*\"roberts\" + 0.005*\"dna\" + 0.005*\"school\"'),\n",
       " (1,\n",
       "  '0.010*\"research\" + 0.008*\"students\" + 0.008*\"hungary\" + 0.007*\"scientists\" + 0.006*\"group\" + 0.006*\"scientist\" + 0.006*\"karikÃ³\" + 0.006*\"goal\" + 0.005*\"bit\" + 0.005*\"school\"'),\n",
       " (2,\n",
       "  '0.015*\"economics\" + 0.011*\"ertl\" + 0.008*\"course\" + 0.008*\"students\" + 0.008*\"chemistry\" + 0.007*\"women\" + 0.005*\"goldin\" + 0.005*\"interested\" + 0.005*\"school\" + 0.005*\"new\"')]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try 3 topics\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=3, id2word=id2wordna, passes=10)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.007*\"biology\" + 0.006*\"hartwell\" + 0.006*\"rna\" + 0.006*\"cell\" + 0.006*\"experiment\" + 0.006*\"roberts\" + 0.006*\"dna\" + 0.005*\"school\" + 0.005*\"research\" + 0.005*\"new\"'),\n",
       " (1,\n",
       "  '0.013*\"research\" + 0.010*\"students\" + 0.010*\"course\" + 0.009*\"group\" + 0.009*\"pÃ¤Ã¤bo\" + 0.008*\"bit\" + 0.005*\"institute\" + 0.005*\"humans\" + 0.005*\"life\" + 0.005*\"example\"'),\n",
       " (2,\n",
       "  '0.021*\"economics\" + 0.015*\"ertl\" + 0.010*\"students\" + 0.010*\"chemistry\" + 0.010*\"women\" + 0.007*\"goldin\" + 0.006*\"new\" + 0.006*\"school\" + 0.006*\"course\" + 0.006*\"surface\"'),\n",
       " (3,\n",
       "  '0.022*\"greengard\" + 0.008*\"drug\" + 0.007*\"research\" + 0.007*\"students\" + 0.007*\"brain\" + 0.006*\"department\" + 0.006*\"pharmacology\" + 0.006*\"biochemistry\" + 0.006*\"ive\" + 0.005*\"companies\"')]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try 4 topics\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=4, id2word=id2wordna, passes=10)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify Topics in Each Document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of the 9 topic models we looked at, the nouns and adjectives, 4 topic one made the most sense. So let's pull that down here and run it through some more iterations to get more fine-tuned topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.011*\"lefkowitz\" + 0.009*\"robert\" + 0.005*\"week\" + 0.005*\"enthusiasm\" + 0.005*\"secret\" + 0.005*\"lefkowitzshare\" + 0.004*\"piano\" + 0.004*\"club\" + 0.004*\"essay\" + 0.004*\"lessons\"'),\n",
       " (1,\n",
       "  '0.009*\"students\" + 0.009*\"research\" + 0.008*\"greengard\" + 0.007*\"new\" + 0.006*\"chemistry\" + 0.006*\"biology\" + 0.005*\"ertl\" + 0.005*\"interested\" + 0.005*\"school\" + 0.005*\"course\"'),\n",
       " (2,\n",
       "  '0.019*\"economics\" + 0.012*\"women\" + 0.007*\"school\" + 0.007*\"hungary\" + 0.007*\"goldin\" + 0.006*\"scientist\" + 0.006*\"scientists\" + 0.006*\"college\" + 0.006*\"goal\" + 0.006*\"karikÃ³\"'),\n",
       " (3,\n",
       "  '0.013*\"pÃ¤Ã¤bo\" + 0.010*\"course\" + 0.008*\"humans\" + 0.007*\"institute\" + 0.007*\"svante\" + 0.005*\"passion\" + 0.005*\"example\" + 0.005*\"today\" + 0.005*\"unique\" + 0.005*\"neandertals\"')]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Our final LDA model (for now)\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=4, id2word=id2wordna, passes=80)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Topic 0 seems to revolve around personal experiences or achievements related to music, piano lessons, clubs, essays, etc.\n",
    "2. Topic 1 appears to be focused on academic research, with terms like \"students,\" \"research,\" \"chemistry,\" \"biology,\" etc., suggesting discussions related to scientific studies and academia.\n",
    "3. Topic 2 likely pertains to economics and social sciences, with terms like \"economics,\" \"women,\" \"school,\" \"college,\" \"scientist,\" etc., indicating discussions related to gender economics, education, and social sciences.\n",
    "4. Topic 3 seems to be about scientific research and institutes, with terms like \"pÃ¤Ã¤bo,\" \"humans,\" \"institute,\" \"svante,\" \"neandertals,\" etc., suggesting discussions related to genetics, human evolution, and research institutes.\n",
    "\n",
    "Based on this analysis, topics 1 and 2 seem to be more coherent and meaningful compared to topics 0 and 3. Topics 1 and 2 cover academic research and economics/social sciences, which are coherent themes, while topics 0 and 3 appear to be less focused or cohesive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'Anne'),\n",
       " (2, 'Claudia'),\n",
       " (1, 'Gerhard'),\n",
       " (2, 'Katalin'),\n",
       " (1, 'Leland'),\n",
       " (1, 'Paul'),\n",
       " (2, 'Pierre'),\n",
       " (1, 'Richard'),\n",
       " (0, 'Robert'),\n",
       " (3, 'Svante')]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's take a look at which topics each transcript contains\n",
    "corpus_transformed = ldana[corpusna]\n",
    "list(zip([a for [(a,b)] in corpus_transformed], data_dtmna.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "For a first pass of LDA, these kind of make sense to me, so we'll call it a day for now.\n",
    "* Topic 0: mom, parents [Anthony, Hasan, Louis, Ricky]\n",
    "* Topic 1: husband, wife [Ali, John, Mike]\n",
    "* Topic 2: guns [Bill, Bo, Jim]\n",
    "* Topic 3: profanity [Dave, Joe]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment:\n",
    "1. Try further modifying the parameters of the topic models above and see if you can get better topics.\n",
    "2. Create a new topic model that includes terms from a different [part of speech](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html) and see if you can get better topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.012*\"pierre\" + 0.007*\"guess\" + 0.007*\"agostinishare\" + 0.005*\"experiment\" + 0.005*\"better\" + 0.005*\"qualities\" + 0.005*\"agostini\" + 0.005*\"literature\" + 0.005*\"experiments\" + 0.005*\"aware\"'),\n",
       " (1,\n",
       "  '0.014*\"hungary\" + 0.012*\"goal\" + 0.012*\"karikÃ³\" + 0.011*\"scientists\" + 0.009*\"scientist\" + 0.007*\"school\" + 0.006*\"rna\" + 0.006*\"katalin\" + 0.006*\"money\" + 0.006*\"vaccine\"'),\n",
       " (2,\n",
       "  '0.035*\"economics\" + 0.018*\"women\" + 0.012*\"goldin\" + 0.009*\"college\" + 0.009*\"economic\" + 0.008*\"students\" + 0.008*\"dog\" + 0.007*\"school\" + 0.007*\"subject\" + 0.007*\"rights\"'),\n",
       " (3,\n",
       "  '0.014*\"greengard\" + 0.008*\"research\" + 0.007*\"roberts\" + 0.006*\"new\" + 0.006*\"experiment\" + 0.006*\"rna\" + 0.005*\"nice\" + 0.005*\"company\" + 0.005*\"biochemistry\" + 0.005*\"ive\"'),\n",
       " (4,\n",
       "  '0.015*\"lefkowitz\" + 0.012*\"robert\" + 0.007*\"week\" + 0.007*\"lefkowitzshare\" + 0.007*\"enthusiasm\" + 0.007*\"secret\" + 0.005*\"piano\" + 0.005*\"id\" + 0.005*\"lessons\" + 0.005*\"club\"'),\n",
       " (5,\n",
       "  '0.011*\"students\" + 0.010*\"ertl\" + 0.009*\"course\" + 0.009*\"research\" + 0.009*\"hartwell\" + 0.007*\"new\" + 0.007*\"biology\" + 0.007*\"cell\" + 0.007*\"chemistry\" + 0.006*\"laboratory\"')]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from gensim import models\n",
    "\n",
    "# Further modify the parameters of the LDA model\n",
    "ldana_modified = models.LdaModel(\n",
    "    corpus=corpusna,  # Your corpus\n",
    "    num_topics=6,  # Change the number of topics\n",
    "    id2word=id2wordna,  # Your id2word mapping\n",
    "    passes=100,  # Increase the number of passes\n",
    "    alpha='auto',  # Use automatic alpha estimation\n",
    "    eta='auto',  # Use automatic eta estimation\n",
    "    random_state=42  # Set a random state for reproducibility\n",
    ")\n",
    "\n",
    "# Print the modified topics\n",
    "ldana_modified.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.012*\"pierre\" + 0.007*\"agostinishare\" + 0.007*\"guess\" + 0.005*\"experiment\" + 0.005*\"qualities\" + 0.005*\"aware\" + 0.005*\"literature\" + 0.005*\"experiments\" + 0.005*\"better\" + 0.005*\"agostini\"'),\n",
       " (1,\n",
       "  '0.014*\"hungary\" + 0.012*\"karikÃ³\" + 0.012*\"goal\" + 0.011*\"scientists\" + 0.009*\"scientist\" + 0.007*\"school\" + 0.006*\"money\" + 0.006*\"rna\" + 0.006*\"katalin\" + 0.006*\"vaccine\"'),\n",
       " (2,\n",
       "  '0.019*\"economics\" + 0.012*\"hartwell\" + 0.010*\"cell\" + 0.010*\"women\" + 0.009*\"biology\" + 0.008*\"school\" + 0.008*\"students\" + 0.008*\"cancer\" + 0.008*\"college\" + 0.007*\"goldin\"'),\n",
       " (3,\n",
       "  '0.013*\"greengard\" + 0.008*\"research\" + 0.007*\"roberts\" + 0.007*\"new\" + 0.006*\"experiment\" + 0.006*\"rna\" + 0.005*\"big\" + 0.005*\"dna\" + 0.005*\"nice\" + 0.005*\"biochemistry\"'),\n",
       " (4,\n",
       "  '0.015*\"lefkowitz\" + 0.012*\"robert\" + 0.007*\"week\" + 0.007*\"secret\" + 0.007*\"enthusiasm\" + 0.007*\"lefkowitzshare\" + 0.005*\"piano\" + 0.005*\"id\" + 0.005*\"essay\" + 0.005*\"club\"'),\n",
       " (5,\n",
       "  '0.017*\"ertl\" + 0.014*\"students\" + 0.013*\"research\" + 0.012*\"course\" + 0.010*\"chemistry\" + 0.009*\"new\" + 0.007*\"interested\" + 0.007*\"bit\" + 0.007*\"pÃ¤Ã¤bo\" + 0.007*\"group\"')]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from gensim import models\n",
    "\n",
    "# Further modify the parameters of the LDA model\n",
    "ldana_modified = models.LdaModel(\n",
    "    corpus=corpusna,  # Your corpus\n",
    "    num_topics=6,  # Change the number of topics\n",
    "    id2word=id2wordna,  # Your id2word mapping\n",
    "    passes=150,  # Increase the number of passes\n",
    "    alpha='auto',  # Use automatic alpha estimation\n",
    "    eta='auto',  # Use automatic eta estimation\n",
    "    random_state=42,  # Set a random state for reproducibility\n",
    "    chunksize=1000,  # Set the chunk size for processing\n",
    "    iterations=200,  # Increase the number of iterations\n",
    "    minimum_probability=0.01,  # Set the minimum probability threshold for a topic\n",
    "    decay=0.5  # Set the decay parameter for learning\n",
    ")\n",
    "\n",
    "# Print the modified topics\n",
    "ldana_modified.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.013*\"greengard\" + 0.010*\"ertl\" + 0.007*\"students\" + 0.006*\"chemistry\" + 0.005*\"going\" + 0.005*\"research\" + 0.005*\"new\" + 0.005*\"theres\" + 0.004*\"interested\" + 0.004*\"drug\"'),\n",
       " (1,\n",
       "  '0.014*\"roberts\" + 0.009*\"going\" + 0.007*\"research\" + 0.006*\"rna\" + 0.005*\"experiment\" + 0.005*\"group\" + 0.005*\"trying\" + 0.005*\"new\" + 0.005*\"id\" + 0.005*\"course\"'),\n",
       " (2,\n",
       "  '0.012*\"hartwell\" + 0.010*\"cell\" + 0.009*\"biology\" + 0.006*\"cancer\" + 0.005*\"going\" + 0.005*\"laboratory\" + 0.004*\"theres\" + 0.004*\"approach\" + 0.004*\"disease\" + 0.004*\"working\"'),\n",
       " (3,\n",
       "  '0.014*\"economics\" + 0.008*\"women\" + 0.006*\"want\" + 0.005*\"goldin\" + 0.005*\"school\" + 0.005*\"karikÃ³\" + 0.005*\"hungary\" + 0.004*\"went\" + 0.004*\"going\" + 0.004*\"started\"')]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's create a function to pull out nouns, adjectives, and verbs from a string of text\n",
    "def nouns_adj_verbs(text):\n",
    "    '''Given a string of text, tokenize the text and pull out only the nouns, adjectives, and verbs.'''\n",
    "    is_noun_adj_verb = lambda pos: pos[:2] in ['NN', 'JJ', 'VB']  # Including verbs ('VB')\n",
    "    tokenized = word_tokenize(text)\n",
    "    nouns_adj_verbs = [word for (word, pos) in pos_tag(tokenized) if is_noun_adj_verb(pos)] \n",
    "    return ' '.join(nouns_adj_verbs)\n",
    "\n",
    "# Apply the nouns_adj_verbs function to the transcripts to filter only on nouns, adjectives, and verbs\n",
    "data_nouns_adj_verbs = pd.DataFrame(data_clean.awardee.apply(nouns_adj_verbs))\n",
    "\n",
    "# Create a new document-term matrix using only nouns, adjectives, and verbs\n",
    "cv_nav = CountVectorizer(stop_words=stop_words, max_df=0.8)\n",
    "data_cv_nav = cv_nav.fit_transform(data_nouns_adj_verbs.awardee)\n",
    "data_dtm_nav = pd.DataFrame(data_cv_nav.toarray(), columns=cv_nav.get_feature_names_out())  # Use get_feature_names_out() instead\n",
    "data_dtm_nav.index = data_nouns_adj_verbs.index\n",
    "\n",
    "# Create the gensim corpus\n",
    "corpus_nav = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(data_dtm_nav.transpose()))\n",
    "\n",
    "# Create the vocabulary dictionary\n",
    "id2word_nav = dict((v, k) for k, v in cv_nav.vocabulary_.items())\n",
    "\n",
    "# Train LDA model with nouns, adjectives, and verbs\n",
    "ldana_nav = models.LdaModel(corpus=corpus_nav, num_topics=4, id2word=id2word_nav, passes=10)\n",
    "ldana_nav.print_topics()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
